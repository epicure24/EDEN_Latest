{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FURIA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRkp8WquPuqo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "028ee36b-dc60-4e67-91f0-9d6d444c375a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx3ReQ9lR9e9"
      },
      "source": [
        "#settings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQJzbW1ZaATF"
      },
      "source": [
        "import subprocess\n",
        "import sys\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "varDataset='Privamov'  # the dataset name\n",
        "varOriginalDatasetPath='/content/drive/My Drive/Mobility datasets/Privamov/'\n",
        "varProjectPath=\"/content/drive/My Drive/Mobility datasets/Experiment/\" # project path , where we will create our project\n",
        "varTrainDays=15 # after trainDays days we begin the daily training ( federated training )\n",
        "varHourOfSplit='23:59:59' # the time when we do the train\n",
        "varFrequence=48 # training frequence, in 30min use case it is equal to 48 because 48*30min = 24h => one train each day\n",
        "\n",
        "#Here we create a csv file which contains path files , lppm names,labels,parameters and colors ( colors are only needed to plot results)\n",
        "data=[]\n",
        "# NOBF case ***** important we have to keep the NOBF case as first case ***\n",
        "varLppmNOBF='NOBF'\n",
        "path=varOriginalDatasetPath+'/Train/NOBF-train-1800.csv' #have to be the first !!!\n",
        "pathTest=varOriginalDatasetPath+'/Test/NOBF-test-1800.csv'\n",
        "varNobfParameter='NOBF-1800'\n",
        "label='NOBF'\n",
        "color='#FF9700'\n",
        "colorLight='#EE981B'\n",
        "data.append([varDataset,varLppmNOBF,varNobfParameter,label,color,colorLight,path,pathTest])\n",
        "\n",
        "# Geo-I cases\n",
        "lppm ='Geo-I'\n",
        "path=varOriginalDatasetPath+'/Train/geoi-train-D48-split--level-15-epsilon-001.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/geoi-test-D48-split--level-15-epsilon-001.csv'\n",
        "param='epsilon-0001'\n",
        "label='Geo-I 0.001'\n",
        "color='#0C3F91'\n",
        "colorLight='#1E55AD'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/geoi-train-D48-split--level-15-epsilon-005.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/geoi-test-D48-split--level-15-epsilon-005.csv'\n",
        "param='epsilon-0005'\n",
        "label='Geo-I 0.005'\n",
        "color='#4981DC'\n",
        "colorLight='#6D9CE6'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/geoi-train-D48-split--level-15-epsilon-01.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/geoi-test-D48-split--level-15-epsilon-01.csv'\n",
        "param='epsilon-001'\n",
        "label='Geo-I 0.001'\n",
        "color='#6CCEFB'\n",
        "colorLight='#97DCFD'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "\n",
        "# Promesse cases\n",
        "varLppmPROM='PROM'\n",
        "path=varOriginalDatasetPath+'/Train/promesse-train-D48-split--level-15-distance-50.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/promesse-test-D48-split--level-15-distance-50.csv'\n",
        "param='distance-50'\n",
        "label='PROM 50m'\n",
        "color='#1B891F'\n",
        "colorLight='#33A03A'\n",
        "data.append([varDataset,varLppmPROM,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/promesse-train-D48-split--level-15-distance-100.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/promesse-test-D48-split--level-15-distance-100.csv'\n",
        "param='distance-100'\n",
        "label='PROM 100m'\n",
        "color='#34C739'\n",
        "colorLight='#55D55D'\n",
        "data.append([varDataset,varLppmPROM,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/promesse-train-D48-split--level-15-distance-200.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/promesse-test-D48-split--level-15-distance-200.csv'\n",
        "param='distance-200'\n",
        "label='PROM 200m'\n",
        "color='#52F258'\n",
        "colorLight='#7BF784'\n",
        "data.append([varDataset,varLppmPROM,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "#TRL Cases\n",
        "lppm='TRL'\n",
        "path=varOriginalDatasetPath+'/Train/tri-train-D48-split--level-15-range-1.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/tri-test-D48-split--level-15-range-1.csv'\n",
        "param='range-1'\n",
        "label='TRL 1km'\n",
        "color='#B82C0F'\n",
        "colorLight='#E04828'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/tri-train-D48-split--level-15-range-2.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/tri-test-D48-split--level-15-range-2.csv'\n",
        "param='range-2'\n",
        "label='TRL 2km'\n",
        "color='#F5340C'\n",
        "colorLight='#FD6241'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "path=varOriginalDatasetPath+'/Train/tri-train-D48-split--level-15-range-3.csv'\n",
        "pathTest=varOriginalDatasetPath+'/Test/tri-test-D48-split--level-15-range-3.csv'\n",
        "param='range-3'\n",
        "label='TRL 3km'\n",
        "color='#FF6A6A'\n",
        "colorLight='#FF8E8E'\n",
        "data.append([varDataset,lppm,param,label,color,colorLight,path,pathTest])\n",
        "\n",
        "# Create the pandas DataFrame\n",
        "varDfInfoParameters = pd.DataFrame(data, columns = ['dataset','lppm','parameter','label','color','colorLight','pathTrain','pathTest'])\n",
        "#dfInfoParameters.to_csv (parametersPath, index = None, header=True)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmOEILgqyI_V"
      },
      "source": [
        "# Dataset preprocessing\n",
        "\n",
        "\n",
        "1.   Convert dataType\n",
        "2.   Uniform features\n",
        "3.   Correct timestamps\n",
        "4.   Reducing and centering\n",
        "5.   Normalize ID\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgDHvt8BqetM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bbaa50d-af7e-40dd-b362-72a7c98c9792"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler ,Binarizer,MaxAbsScaler,Normalizer,PowerTransformer,RobustScaler\n",
        "import glob\n",
        "from datetime import datetime \n",
        "from datetime import timedelta\n",
        "import sys\n",
        "from sklearn import preprocessing\n",
        "import time\n",
        "\n",
        "\"\"\"#Settings\"\"\"\n",
        "dataset= varDataset # the dataset name\n",
        "trainDays=varTrainDays# number of days to begin the daily training\n",
        "originalDatasetPath=varOriginalDatasetPath# path to datasets\n",
        "SavePath=varProjectPath# project path , where we will create our project\n",
        "hourOfSplit=varHourOfSplit    # the time when we do the train\n",
        "frequence=varFrequence # training frequence, in 30min use case it is equal to 48 because 48*30min = 24h => one train each day\n",
        "dfInfoParameters = varDfInfoParameters\n",
        "lppmNOBF=varLppmNOBF\n",
        "\n",
        "# Global variables\n",
        "allParameters=[]\n",
        "lppmNames=[]\n",
        "DatasetPath=SavePath+'/Prepared_Datasets/'+dataset+'/'\n",
        "\n",
        "def convertToFloat32(df):\n",
        "  IDs = df['IDs'].copy()\n",
        "  timestamp=df['timestamp'].copy()\n",
        "  del df['timestamp']\n",
        "  del df['IDs']\n",
        "  df=df.astype('float32')\n",
        "  df['IDs']=IDs\n",
        "  df['timestamp']=timestamp\n",
        "  return df\n",
        "def normalizeIDwithTable(df,table):\n",
        "  df['oldIDs'] = df['IDs']\n",
        "  df['IDs'] = df['IDs'].apply(lambda x: str(x).replace('-','_').split('_')[0])\n",
        "  try : \n",
        "    df['IDs']=df ['IDs'].astype('int')\n",
        "    df=df.sort_values(['IDs'])\n",
        "  except : \n",
        "    print(\" WARNING DATASET IS NOT INT TYPE \\n\")\n",
        "  finally :  \n",
        "    uniqueTrain=df['IDs'].unique()\n",
        "    for uniqueID in uniqueTrain :\n",
        "      df['IDs']= df['IDs'].replace(to_replace=uniqueID,value=tableNewOldID[uniqueID])\n",
        "    return df\n",
        "def normalizeID(train) :\n",
        "  tableNewOldID = {}\n",
        "  cptID=0\n",
        "  train['oldIDs'] = train['IDs']\n",
        "  train['IDs'] = train['IDs'].apply(lambda x: str(x).replace('-','_').split('_')[0])\n",
        "  try : \n",
        "    train['IDs']=train ['IDs'].astype('int')\n",
        "    train=train.sort_values(['IDs'])\n",
        "  except : \n",
        "    print(\" WARNING DATASET IS NOT INT TYPE \\n\")\n",
        "  finally :  \n",
        "    uniqueTrain=train['IDs'].unique()\n",
        "    for uniqueID in uniqueTrain :\n",
        "      train['IDs']= train['IDs'].replace(to_replace=uniqueID,value=cptID)\n",
        "      tableNewOldID[uniqueID]=cptID\n",
        "      cptID=cptID+1\n",
        "    return [train,tableNewOldID]\n",
        "\n",
        "def ReducingCentering(dfUnion) :\n",
        "  dataUnion = dfUnion.values \n",
        "  sc = StandardScaler() \n",
        "  #transformation – centrage-réduction \n",
        "  dataNormalize = sc.fit_transform(dataUnion)\n",
        "  dfUnion = pd.DataFrame(dataNormalize,columns=dfUnion.head(0).columns.values) \n",
        "  return dfUnion\n",
        "\n",
        "def removeFeatures(df,featuresList):\n",
        "  df=df.drop(featuresList, axis = 1) \n",
        "  return df\n",
        "\n",
        "def removeAdditionalFeatures(df,featuresList,FeatureImportant):\n",
        "  listToRemove=np.setdiff1d(featuresList, FeatureImportant)\n",
        "  df=removeFeatures(df,listToRemove)\n",
        "  return df\n",
        "\n",
        "columnLists=[]\n",
        "pathLists=[]\n",
        "\n",
        "dfNobf=dfInfoParameters[dfInfoParameters['lppm']==lppmNOBF]\n",
        "for idx,pathRow in dfNobf.iterrows() :\n",
        "  pathLists.append(pathRow['pathTrain'])\n",
        "  pathLists.append(pathRow['pathTest'])\n",
        "\n",
        "#read headers only and got an union of all features\n",
        "for pathFile in pathLists:\n",
        "  columnList=pd.read_csv(pathFile, nrows=0).columns.tolist()\n",
        "  columnLists=list(set().union(columnLists,columnList))\n",
        "\n",
        "\n",
        "#add zeros and concat and save\n",
        "for idx,dfRow in dfInfoParameters.iterrows():\n",
        "  fileColumns=pd.read_csv(dfRow['pathTrain'], nrows=0).columns.tolist()\n",
        "  ColumnsToRead=np.intersect1d(columnLists,fileColumns)\n",
        "  dfTrain=pd.read_csv(dfRow['pathTrain'],usecols=ColumnsToRead)\n",
        "  columnNames=dfTrain.columns.tolist()\n",
        "  dfTrain=removeAdditionalFeatures(dfTrain,columnNames,columnLists)\n",
        "  fileColumns=pd.read_csv(dfRow['pathTest'], nrows=0).columns.tolist()\n",
        "  ColumnsToRead=np.intersect1d(columnLists,fileColumns)\n",
        "  dfTest=pd.read_csv(dfRow['pathTest'],usecols=ColumnsToRead)\n",
        "  columnNames=dfTest.columns.tolist()\n",
        "  dfTest=removeAdditionalFeatures(dfTest,columnNames,columnLists)\n",
        "  #concat train and test \n",
        "  df=pd.concat([dfTrain,dfTest])\n",
        "  del dfTrain\n",
        "  del dfTest\n",
        "  print(\"Concatenation finished\")\n",
        "  df = df.fillna(0)\n",
        "  df=df.reset_index(drop=True)\n",
        "  \n",
        "  #unify features\n",
        "  df=convertToFloat32(df)\n",
        "  print(\"converted to float 32\")\n",
        "  columnNames=df.columns.tolist()\n",
        "  columnToAdd=list(set(columnLists) - set(columnNames))\n",
        "  zeros=np.zeros(df.shape[0],dtype='uint8')\n",
        "  for idx,column in enumerate(columnToAdd) :\n",
        "    df[column]=zeros\n",
        "  df = df.sort_index(axis=1)\n",
        "  print(\"Uniform features finished\")\n",
        "\n",
        "  #correct timestamps (unify all timestamps with NOBF timestamps)\n",
        "  lppm=dfRow['lppm']\n",
        "  param=dfRow['parameter']\n",
        "  if (lppm==lppmNOBF) :\n",
        "    dfNOBF_timestamp_ids=df[['IDs','timestamp']].copy()\n",
        "  else : \n",
        "    del df['timestamp']\n",
        "    df=pd.merge(df, dfNOBF_timestamp_ids, how='inner', on=['IDs'])\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "  #Reducing and centering data\n",
        "  IDs=df['IDs'].copy()\n",
        "  total=df['total'].copy()\n",
        "  Timestamps=df['timestamp'].copy()\n",
        "  df.pop('timestamp')\n",
        "  df.pop('IDs')\n",
        "  df.pop('total') \n",
        "  df= ReducingCentering(df)\n",
        "  df['IDs']=IDs\n",
        "  df['timestamp']=Timestamps\n",
        "  df['total']=total\n",
        "  print(\" Reduction centering finished\")\n",
        "\n",
        "\n",
        "  #transform the IDs format and store the old one's on oldIDs\n",
        "  if(lppm==lppmNOBF) :\n",
        "    [df,tableNewOldID]=normalizeID(df)\n",
        "  else :\n",
        "    df=normalizeIDwithTable(df,tableNewOldID)\n",
        "  print(\" format IDs finished\")\n",
        "\n",
        "  #save the formated dataset\n",
        "  if not os.path.exists(DatasetPath):\n",
        "    os.makedirs(DatasetPath)\n",
        "  SaveDatasetPath=DatasetPath+dataset+'-'+lppm+'-'+param+'.csv'\n",
        "  df.to_pickle(SaveDatasetPath, compression='zip')\n",
        "  print(\"save\",df.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (4883, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (3200, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (1601, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n",
            "Concatenation finished\n",
            "converted to float 32\n",
            "Uniform features finished\n",
            " Reduction centering finished\n",
            " format IDs finished\n",
            "save (6554, 5260)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZRhbELI1wYH"
      },
      "source": [
        "# Generate DRk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YjhUeie1wpE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf42d9c3-2bdf-4015-e048-7ba919845a58"
      },
      "source": [
        "\"\"\"# Generate DRk\"\"\"\n",
        "import glob\n",
        "import pandas as pd\n",
        "from datetime import datetime \n",
        "from datetime import timedelta\n",
        "import shutil\n",
        "import os\n",
        "import sys\n",
        "\n",
        "\n",
        "\n",
        "trainDays=varTrainDays\n",
        "dfInfoParameters=varDfInfoParameters\n",
        "dataset=varDataset\n",
        "ProjectPath=varProjectPath\n",
        "hourOfSplit=varHourOfSplit\n",
        "frequence=varFrequence\n",
        "lppmNOBF=varLppmNOBF\n",
        "DatasetPath=ProjectPath+'/Prepared_Datasets/'+dataset+'/'\n",
        "\n",
        "def prepareDataset (datasetName,file,trainDays,dirDatasetTrainAfk,hourOfSplit='23:59:59',frequence=48) :\n",
        "    if not os.path.exists(DatasetPath):\n",
        "        os.makedirs(DatasetPath)\n",
        "    if not os.path.exists(dirDatasetTrainAfk):\n",
        "        os.makedirs(dirDatasetTrainAfk)\n",
        "    dfUnion=pd.read_pickle(file,compression='zip')\n",
        "    n_class=dfUnion['IDs'].nunique()\n",
        "  \n",
        "    #----------------Extract the train AF0/AC0-----------------------\n",
        "    dfUnion=dfUnion.sort_values(['timestamp'])\n",
        "    firstTraceDate = dfUnion.iloc[0]['timestamp'] \n",
        "    try : \n",
        "      firstTraceDate_obj = datetime.strptime(firstTraceDate, '%Y-%m-%d %H:%M:%S')\n",
        "      splitDate = firstTraceDate_obj + timedelta(days=trainDays)  \n",
        "      timestampSplit=splitDate.strftime('%Y-%m-%d %H:%M:%S')\n",
        "    except : \n",
        "      print(\"timestamp format error\")\n",
        "      firstTraceDate_obj = datetime.strptime(firstTraceDate, '%Y-%m-%d %H:%M:%S.%f')\n",
        "      splitDate = firstTraceDate_obj + timedelta(days=trainDays)  \n",
        "      timestampSplit=splitDate.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
        "  \n",
        "\n",
        "    #Split the dataframe dfUnion into training and testing set\n",
        "    dfTest = dfUnion[dfUnion['timestamp']>timestampSplit]\n",
        "    dfTrain = dfUnion[dfUnion['timestamp']<=timestampSplit]\n",
        "    dfTest=dfTest.reset_index(drop=True)\n",
        "    \n",
        "    timestampSplit = splitDate.strftime('%Y-%m-%d')\n",
        "    timestampSplit = timestampSplit +\" \"+hourOfSplit\n",
        "\n",
        "    #Split the dataframe dfTest into DR1 and DRK k >1 to ajust time split\n",
        "    dfDR1 = dfTest[dfTest['timestamp']<=timestampSplit]\n",
        "    dfTest = dfTest[dfTest['timestamp']>timestampSplit]\n",
        "    dfTest=dfTest.reset_index(drop=True)\n",
        "\n",
        "  \n",
        "    #save DR0 and IR0\n",
        "\n",
        "    dfTrain= dfTrain.sort_values(['IDs'])\n",
        "    InfoTrain=dfTrain[['IDs','oldIDs','timestamp']]\n",
        "    InfoTrain.to_csv(dirDatasetTrainAfk+\"/IR0.csv\") \n",
        "    dfTrain.pop('oldIDs')\n",
        "    dfTrain.pop('timestamp')\n",
        "    dfTrain.to_pickle(dirDatasetTrainAfk+\"/DR0.csv\",compression='zip') \n",
        "    n_features = dfTrain.shape[1] \n",
        "\n",
        "    #save DR1 and IR1\n",
        "\n",
        "    dfDR1= dfDR1.sort_values(['IDs'])\n",
        "    InfoTrain=dfDR1[['IDs','oldIDs','timestamp']]\n",
        "    InfoTrain.to_csv(dirDatasetTrainAfk+\"/IR1.csv\") \n",
        "    dfDR1.pop('oldIDs')\n",
        "    dfDR1.pop('timestamp')\n",
        "    dfDR1.to_pickle(dirDatasetTrainAfk+\"/DR1.csv\",compression='zip') \n",
        "    testFile=dfTest['oldIDs']\n",
        "\n",
        "    #-----------------------------------------------generate DRk datasets (k > 0) -------------------------------------------------\n",
        "    round = 1   \n",
        "    traceList = ((testFile.apply(lambda x: str(x).replace('-','_').split('_')[1])).values).astype(int)\n",
        "    currentTrace=traceList[0]\n",
        "    traceNumberRound = 0\n",
        "    for (idx,trace) in enumerate(traceList) :\n",
        "      if((currentTrace+frequence)>trace) :\n",
        "        traceNumberRound = traceNumberRound + 1\n",
        "      else :\n",
        "        round = round + 1\n",
        "        df2Split = dfTest.loc[(idx-traceNumberRound):(idx-1),:]\n",
        "        #saveTimesTamp and oldIDS\n",
        "        df2Split= df2Split.sort_values(['IDs'])\n",
        "        dfSplitInfo=df2Split[['IDs','oldIDs','timestamp']]\n",
        "        dfSplitInfo.to_csv(dirDatasetTrainAfk+\"/IR\"+str(round)+\".csv\") \n",
        "        df2Split.pop('timestamp')\n",
        "        df2Split.pop('oldIDs')\n",
        "        df2Split.to_pickle (dirDatasetTrainAfk+\"/DR\"+str(round)+\".csv\",compression='zip') \n",
        "        traceNumberRound = 1 \n",
        "        currentTrace=trace\n",
        "    #Save the rest \n",
        "    round = round + 1\n",
        "    df2Split = dfTest.loc[(idx-traceNumberRound):(idx-1),:]\n",
        "    df2Split= df2Split.sort_values(['IDs'])\n",
        "    timestamptestFile=df2Split[['IDs','oldIDs','timestamp']]\n",
        "    timestamptestFile.to_csv(dirDatasetTrainAfk+\"/IR\"+str(round)+\".csv\") \n",
        "    df2Split.pop('timestamp')\n",
        "    df2Split.pop('oldIDs')\n",
        "    df2Split.to_pickle(dirDatasetTrainAfk+\"/DR\"+str(round)+\".csv\",compression='zip') \n",
        "    return [n_class,n_features]\n",
        "\n",
        "allParameters=dfInfoParameters['parameter']\n",
        "for param in allParameters :\n",
        "  print(\"*************************************************\")\n",
        "  print(param)\n",
        "  print(DatasetPath)\n",
        "  fileToDevide=glob.glob(DatasetPath+'*'+param+'.csv')\n",
        "  print(fileToDevide[0])\n",
        "  directoryToSave=DatasetPath+'/'+param\n",
        "  print(\"before preparedataset function\")\n",
        "  [n_class,n_features]=prepareDataset (dataset,fileToDevide[0],trainDays,directoryToSave,hourOfSplit=hourOfSplit,frequence=frequence) \n",
        "  print(n_class)\n",
        "  print(n_features)\n",
        "  if(param == dfInfoParameters[dfInfoParameters['lppm']==lppmNOBF]['parameter'].values.item()):\n",
        "    DatasetInfoPath=DatasetPath+'/DatasetInfo.csv'\n",
        "    dfInfo = pd.DataFrame(data=[[dataset,n_class,n_features]], columns = ['Dataset', 'classNumber','featureNumber']) \n",
        "    dfInfo.to_pickle (DatasetInfoPath,compression='zip')\n",
        "    print(dfInfo)\n",
        "\n",
        "\n",
        "'''Split each DRk per User'''\n",
        "\n",
        "#separate in a DRk , users : \n",
        "inputDirectory=varProjectPath+\"/\"+\"Prepared_Datasets/\"+dataset+\"/\"+\"NOBF-1800\"\n",
        "outputDirectory=varProjectPath+\"/\"+\"Prepared_Datasets/\"+dataset+\"/\"+\"NOBF-1800-split\"\n",
        "\n",
        "#extract the number of rounds\n",
        "path =inputDirectory+'/DR*.csv'\n",
        "filesList=glob.glob(path)   \n",
        "numberOfRound = len(filesList)\n",
        "\n",
        "if not os.path.exists(outputDirectory):\n",
        "\tos.makedirs(outputDirectory)\n",
        "\n",
        "df=pd.read_pickle(inputDirectory+'/DR0.csv',compression='zip')\n",
        "\n",
        "for i in range(0,numberOfRound):\n",
        "\tprint(\"round\",i)\n",
        "\tdf=pd.read_pickle(inputDirectory+'/DR'+str(i)+'.csv',compression='zip')\n",
        "\tids=df['IDs'].tolist()\n",
        "\tids_unique= list(set(ids))\n",
        "\tgrouped = df.groupby(df.IDs)\n",
        "\n",
        "\tfor user in ids_unique:\n",
        "\t\tuser_df = grouped.get_group(user)\n",
        "\t\tuser_df=user_df.reset_index(drop=True)\n",
        "\t\tif not os.path.exists(outputDirectory+'/DR'+str(i)+'/'):\n",
        "\t\t\tos.makedirs(outputDirectory+'/DR'+str(i)+'/')\n",
        "\t\tuser_df.to_pickle(outputDirectory+'/DR'+str(i)+'/'+str(user)+'.csv',compression='zip')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    dataset  ...                                           pathTest\n",
            "0  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "1  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "2  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "3  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "4  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "5  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "6  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "7  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "8  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "9  Privamov  ...  /content/drive/My Drive/Mobility datasets/Priv...\n",
            "\n",
            "[10 rows x 8 columns]\n",
            "*************************************************\n",
            "NOBF-1800\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-NOBF-NOBF-1800.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "    Dataset  classNumber  featureNumber\n",
            "0  Privamov           48           5258\n",
            "*************************************************\n",
            "epsilon-0001\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-Geo-I-epsilon-0001.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "*************************************************\n",
            "epsilon-0005\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-Geo-I-epsilon-0005.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "*************************************************\n",
            "epsilon-001\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-Geo-I-epsilon-001.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "*************************************************\n",
            "distance-50\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-PROM-distance-50.csv\n",
            "before preparedataset function\n",
            "45\n",
            "5258\n",
            "*************************************************\n",
            "distance-100\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-PROM-distance-100.csv\n",
            "before preparedataset function\n",
            "43\n",
            "5258\n",
            "*************************************************\n",
            "distance-200\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-PROM-distance-200.csv\n",
            "before preparedataset function\n",
            "42\n",
            "5258\n",
            "*************************************************\n",
            "range-1\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-TRL-range-1.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "*************************************************\n",
            "range-2\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-TRL-range-2.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "*************************************************\n",
            "range-3\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/\n",
            "/content/drive/My Drive/Mobility datasets/Experiment//Prepared_Datasets/Privamov/Privamov-TRL-range-3.csv\n",
            "before preparedataset function\n",
            "48\n",
            "5258\n",
            "round 0\n",
            "round 1\n",
            "round 2\n",
            "round 3\n",
            "round 4\n",
            "round 5\n",
            "round 6\n",
            "round 7\n",
            "round 8\n",
            "round 9\n",
            "round 10\n",
            "round 11\n",
            "round 12\n",
            "round 13\n",
            "round 14\n",
            "round 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwZ4pTFp2QqG"
      },
      "source": [
        "# Generate models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PPI4qVK2XRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d2bb7c-757c-4749-dc75-d5015e6aea7c"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "from datetime import datetime\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import glob\n",
        "import sys\n",
        "import os\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "\"\"\"#Config Project\n",
        "\n",
        "## Parameters\n",
        "\"\"\"\n",
        "dataset= varDataset # the dataset name\n",
        "projectPath= varProjectPath #path where we have created our project\n",
        "epoch= 1000# the maximum number of epochs we can do ( federated and cetralized version)\n",
        "batch= 1 # batch size\n",
        "lr= 0.001 # learning rate value for the simple gradient descent optimizer\n",
        "Round = 0 # from wich round we begin the train ( if first time set Round = 0 )\n",
        "#parametersPath = sys.argv[10]\n",
        "dfInfoParameters=varDfInfoParameters\n",
        "\"\"\"Global variables\"\"\"\n",
        "\n",
        "#dfInfoParameters=pd.read_csv(parametersPath)\n",
        "NOBFParameter=  dfInfoParameters[dfInfoParameters['lppm']=='NOBF']['parameter'].values.item()\n",
        "allParameters=dfInfoParameters['parameter']\n",
        "DatasetInfoPath=projectPath+'Prepared_Datasets/'+dataset+'/DatasetInfo.csv'\n",
        "ResultAccuracyPath=projectPath+'/Evaluation/'+dataset+'/Results/'\n",
        "ModelsPath = projectPath+'/Models/'+dataset+'/'\n",
        "pathDRKC=projectPath+'Prepared_Datasets/'+dataset+'/NOBF-1800/'  \n",
        "pathDRKF=projectPath+'Prepared_Datasets/'+dataset+'/NOBF-1800-split/'  \n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = self.list_IDs[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "\"\"\"## Load dataset\"\"\"\n",
        "\n",
        "def getInfo(datasetName,columnName) : \n",
        "  dfInfo = pd.read_pickle(DatasetInfoPath,compression='zip')\n",
        "  dfInfo = dfInfo[dfInfo['Dataset'] == datasetName]\n",
        "  info = dfInfo[columnName].values.item(0)\n",
        "  return info\n",
        "\n",
        "\n",
        "def loadDataset(datafile):\n",
        "  df = pd.read_pickle(datafile,compression='zip')#.fillna(0)\n",
        "  training_target = df['IDs'].values\n",
        "  df.pop('IDs')\n",
        "  factor=df['total'].values\n",
        "  df.pop('total') # besma poping\n",
        "  training_features=df.values\n",
        "  training_features=np.asarray(training_features, dtype=np.float32)\n",
        "  training_target = np.asarray(training_target, dtype=np.int16)\n",
        "  return training_features, training_target,factor\n",
        "\n",
        "\n",
        "#Create a data loader for  centralized dataset \n",
        "def createDataLoader(inputs,labels,train):\n",
        "  inputs_torch = torch.tensor(inputs,dtype=torch.float, requires_grad=True).cuda()\n",
        "  labels_torch = torch.tensor(labels,dtype=torch.long).cuda()\n",
        "  Dataset_set = Dataset(inputs_torch, labels_torch)\n",
        "  if train :\n",
        "    loader = torch.utils.data.DataLoader(dataset=Dataset_set,batch_size=len(labels),shuffle=True)\n",
        "  else :\n",
        "    loader = torch.utils.data.DataLoader(dataset=Dataset_set,batch_size=1,shuffle=False)\n",
        "  return loader\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    input_dim: number of input features.\n",
        "    output_dim: number of labels.\n",
        "    \"\"\"\n",
        "    super(LogisticRegressionModel, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    outputs = self.linear(x)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def getModel(datasetName,round=0) : \n",
        "  n_feature = getInfo(datasetName,'featureNumber')-1-1 #remove labels column and total col\n",
        "  n_class = getInfo(datasetName,'classNumber')\n",
        "  model = LogisticRegressionModel(n_feature, n_class).cuda()\n",
        "  if(round > 0) :\n",
        "    modelPath = ModelsPath+'/AF'+str(round-1)+'.pt'\n",
        "    model = torch.load(modelPath)\n",
        "  return model\n",
        "\n",
        "def local_train(model, device, federated_train_loader, optimizer):\n",
        "  model.train()\n",
        "  lossFunction = nn.CrossEntropyLoss() #weights\n",
        "    # Iterate through each gateway's dataset\n",
        "  for idx, (data, target) in enumerate(federated_train_loader):\n",
        "      batch_idx = idx+1\n",
        "      # Move the data and target labels to the device (cpu/gpu) for computation\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      # Clear previous gradients (if they exist)\n",
        "      optimizer.zero_grad()\n",
        "      # Make a prediction\n",
        "      output = model(data)\n",
        "      loss= lossFunction(output,target)\n",
        "      # Calculate the gradients\n",
        "      loss.backward()\n",
        "      # Update the model weights\n",
        "      optimizer.step()\n",
        "      # Get the model back from the gateway\n",
        "  return [model,loss.item()]\n",
        "\n",
        "\n",
        "\n",
        "def generateAFK(model,dataset,round,modelDirectory,resultDirectory, X_train, y_train,EPOCHS = 10,BATCH_SIZE =1,lr = 0.0001,firstUser=False) :  \n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "  scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=0.01)\n",
        "  epoch=0\n",
        "  federated_train_loader=createDataLoader(X_train,y_train,True)\n",
        "  \n",
        "  while (epoch < EPOCHS):\n",
        "      [model,loss]=local_train(model, device, federated_train_loader, optimizer) #,weights\n",
        "      epoch=epoch +1\n",
        "  model_copy=model\n",
        "  return model_copy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ************************************* Run Experimentation *************************************#\n",
        "##################################################################################################\n",
        "\n",
        "#here we precise if we will use cpu or cuda ( for cuda , we should add other modifications)\n",
        "device = torch.device(\"cuda\")\n",
        "n_feature = getInfo(dataset,'featureNumber')-1-1 #remove labels column and total col\n",
        "n_class = getInfo(dataset,'classNumber')\n",
        "\n",
        "\n",
        "\n",
        "#extract the number of rounds\n",
        "path =pathDRKF+'/DR*'\n",
        "filesList=glob.glob(path)   \n",
        "NumRounds = len(filesList)\n",
        "\n",
        "modelF=getModel(dataset,Round)\n",
        "\n",
        "if not os.path.exists(ModelsPath):\n",
        "  os.makedirs(ModelsPath)\n",
        "\n",
        "while (Round < NumRounds) :  \n",
        "  print(\"-----------------------------------------------------------------------------------------\")\n",
        "  print(\"Round  :\" + str(Round  ) ) \n",
        "  print(\"------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  modelPathResult=ModelsPath+'AF'+str(Round)+'.pt'\n",
        "  for idx,filename in enumerate(os.listdir(pathDRKF+'DR'+str(Round)+'/')):\n",
        "    if idx==0: \n",
        "      firstUser=True\n",
        "    else: \n",
        "      firstUser=False\n",
        "    nb_current_users=len([name for name in os.listdir(pathDRKF+'DR'+str(Round)+'/') if os.path.isfile(os.path.join(pathDRKF+'DR'+str(Round)+'/', name))])\n",
        "    user_name=filename.split('.')[0]\n",
        "    trainFile=pathDRKF+'DR'+str(Round)+'/'+filename\n",
        "    X_train, y_train,factor= loadDataset(trainFile)\n",
        "    resultDirectory=ResultAccuracyPath+NOBFParameter+'/'            \n",
        "    modelF=generateAFK(modelF,dataset,Round,modelPathResult,resultDirectory, X_train, y_train,EPOCHS = epoch,BATCH_SIZE =len(y_train),lr = lr,firstUser=firstUser)   \n",
        "  torch.save(modelF,modelPathResult)\n",
        "  Round = Round + 1"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------------------------------\n",
            "Round  :0\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :1\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :2\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :3\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :4\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :5\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :6\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :7\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :8\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :9\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :10\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :11\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :12\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :13\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :14\n",
            "------------------------------------------------------------------------------------------\n",
            "-----------------------------------------------------------------------------------------\n",
            "Round  :15\n",
            "------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELTogJfeMzE2"
      },
      "source": [
        "#Test models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf8YV1STM207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "439fa09d-6887-4064-b845-8dd78d7a0d32"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import sys\n",
        "import glob\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\"\"\"#Settings\"\"\"\n",
        "\n",
        "dataset= varDataset\n",
        "pathProject= varProjectPath\n",
        "dfInfoParameters=varDfInfoParameters\n",
        "NOBFParameters=varNobfParameter\n",
        "promesseParameters=dfInfoParameters[dfInfoParameters['lppm']==varLppmPROM]['parameter'].values\n",
        "pathDRKC=pathProject+'Prepared_Datasets/'+dataset+'/NOBF-1800/'\n",
        "path =pathDRKC+'/DR*.csv'\n",
        "filesList=glob.glob(path)   \n",
        "nb_rounds = len(filesList)-1 \n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    \"\"\"\n",
        "    input_dim: number of input features.\n",
        "    output_dim: number of labels.\n",
        "    \"\"\"\n",
        "    super(LogisticRegressionModel, self).__init__()\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)\n",
        "  def forward(self, x):\n",
        "    outputs = self.linear(x)#F.softmax(x,dim=1)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "\n",
        "    def __init__(self, list_IDs, labels):\n",
        "        'Initialization'\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load data and get label\n",
        "        X = self.list_IDs[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "def loadTest(testFile):\n",
        "  datafile2 = testFile\n",
        "  df2 = pd.read_pickle(datafile2,compression='zip')\n",
        "  y2 = df2['IDs'].values\n",
        "  df2.pop('IDs')\n",
        "  df2.pop('total')  \n",
        "  x2=df2.values\n",
        "  x2=np.asarray(x2, dtype=np.float32)\n",
        "  y2 = np.asarray(y2, dtype=np.float32)\n",
        "  n_features= x2.shape[1]\n",
        "  testing_features, testing_target  = x2, y2\n",
        "  user_count_dict = Counter(testing_target)\n",
        "  user_types = list(user_count_dict.keys())\n",
        "  return len(user_types),n_features,testing_features, testing_target\n",
        "\n",
        "def loadModel(n_feature,n_class,pathmodel):\n",
        "  model_new = LogisticRegressionModel(n_feature,n_class).cuda()\n",
        "  model_new=torch.load(pathmodel) \n",
        "  return model_new\n",
        "\n",
        "#Create a data loader for  centralized dataset \n",
        "def createDataLoader(inputs,labels):\n",
        "  inputs_torch = torch.tensor(inputs,dtype=torch.float, requires_grad=True).cuda()\n",
        "  labels_torch = torch.tensor(labels,dtype=torch.long).cuda()\n",
        "  Dataset_set = Dataset(inputs_torch, labels_torch)\n",
        "  loader = torch.utils.data.DataLoader(dataset=Dataset_set,batch_size=1,shuffle=False)\n",
        "  return loader\n",
        "\n",
        "\n",
        "def testdecentralized(model,test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    df = pd.DataFrame(columns=['target','prediction','origin-target'])\n",
        "    for data, target in test_loader:\n",
        "          # Forward pass only to get logits/output\n",
        "          outputs = model(data)\n",
        "          # Get predictions from the maximum value\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          # Total number of labels\n",
        "          total += target.size(0)\n",
        "          # Total correct predictions\n",
        "          correct += (predicted == target).sum()  \n",
        "          #Save prediction in csv file \n",
        "          predicted=predicted.item()\n",
        "          target=target.item()  \n",
        "          df = df.append({'target':target, 'prediction':predicted}, ignore_index=True)\n",
        "    return correct.item(),total,df\n",
        "\n",
        "def concat_save(df_pred,df_IR,pathPrediction):\n",
        "  #concatener IRk et pred-k\n",
        "  df_pred['origin-target']=df_IR['oldIDs']\n",
        "  df_pred.to_csv(pathPrediction, index=False)\n",
        "\n",
        "def result_save(df,pathResult):\n",
        "  df.to_csv(pathResult, index=False)\n",
        "\n",
        "#**************************  Load data ***********************************#\n",
        "\n",
        "dfInfo = pd.read_pickle(pathProject+'Prepared_Datasets/'+dataset+'/DatasetInfo.csv',compression='zip')\n",
        "nb_users = dfInfo[dfInfo['Dataset'] == dataset]['classNumber'].values.item(0)\n",
        "i=0\n",
        "df = pd.DataFrame(columns=['Dataset','Lppm','Model','Round','Nb_user','Correct','Total','Accuracy'])\n",
        "pathResult=pathProject+'/Evaluation/'+dataset+'/Result_final_'+dataset+'.csv'\n",
        "totalnobf=[]\n",
        "for lppm in dfInfoParameters['parameter']:\n",
        "  directory=pathProject+'/Evaluation/'+dataset+'/Predictions/'+lppm\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "  for nb_round in range(0,nb_rounds):\n",
        "    print(\"round: \", nb_round)\n",
        "    testFile=pathProject+'/Prepared_Datasets/'+dataset+'/'+lppm+'/DR'+str(nb_round+1)+'.csv'  \n",
        "    pathIRk=pathProject+'/Prepared_Datasets/'+dataset+'/'+lppm+'/IR'+str(nb_round+1)+'.csv'\n",
        "    pathPrediction2=pathProject+'/Evaluation/'+dataset+'/Predictions/'+lppm+'/prediction-decentralized-'+str(nb_round+1)+'.csv' \n",
        "    pathmodel_decentralized=pathProject+'/Models/'+dataset+'/AF'+str(nb_round)+'.pt'  # Running : \n",
        "    #*******************************************************************************#\n",
        "    # Test federated model with simulation \n",
        "    #*******************************************************************************#\n",
        "    nb_users_test, n_features, X_test, y_test = loadTest(testFile)\n",
        "    if lppm=='NOBF-1800':\n",
        "      totalnobf.append(len(y_test))\n",
        "    #2- Load the federated model AF\n",
        "    federated_model= loadModel(n_features,nb_users,pathmodel_decentralized) \n",
        "    test_loader=createDataLoader(X_test,y_test)\n",
        "    correct2,total2,df_pred2=testdecentralized(federated_model,test_loader)\n",
        "    print(lppm,dataset)\n",
        "    print(\"decentralized accuracy\", correct2,totalnobf[nb_round],correct2/totalnobf[nb_round])\n",
        "    #3- Prepare prediction file\n",
        "    df_IR = pd.read_csv(pathIRk)\n",
        "    concat_save(df_pred2,df_IR,pathPrediction2)\n",
        "    #'Dataset','Lppm','Model','Round','Nb_user','Correct','Total','Accuracy'])\n",
        "    df = df.append({'Dataset':dataset, 'Lppm':lppm,'Model':'AF'+str(nb_round),'Round':str(nb_round+1),'Nb_user':nb_users_test,'Correct':correct2,'Total':totalnobf[nb_round],'Accuracy':correct2/totalnobf[nb_round]}, ignore_index=True)\n",
        "   \n",
        "result_save(df,pathResult)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "round:  0\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 137 189 0.7248677248677249\n",
            "round:  1\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 200 210 0.9523809523809523\n",
            "round:  2\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 204 247 0.8259109311740891\n",
            "round:  3\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 84 132 0.6363636363636364\n",
            "round:  4\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 48 51 0.9411764705882353\n",
            "round:  5\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 105 137 0.7664233576642335\n",
            "round:  6\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 125 133 0.9398496240601504\n",
            "round:  7\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 138 205 0.6731707317073171\n",
            "round:  8\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 195 287 0.6794425087108014\n",
            "round:  9\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 116 209 0.5550239234449761\n",
            "round:  10\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 98 139 0.7050359712230215\n",
            "round:  11\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 55 137 0.40145985401459855\n",
            "round:  12\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 141 249 0.5662650602409639\n",
            "round:  13\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 99 163 0.6073619631901841\n",
            "round:  14\n",
            "NOBF-1800 Privamov\n",
            "decentralized accuracy 195 226 0.8628318584070797\n",
            "round:  0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-5a1a9fd246a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m# Test federated model with simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m#*******************************************************************************#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mnb_users_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloadTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlppm\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'NOBF-1800'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m       \u001b[0mtotalnobf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-5a1a9fd246a5>\u001b[0m in \u001b[0;36mloadTest\u001b[0;34m(testFile)\u001b[0m\n\u001b[1;32m     54\u001b[0m   \u001b[0mdatafile2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m   \u001b[0my2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'IDs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m   \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IDs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'total'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2876\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2877\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2878\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2880\u001b[0m         \u001b[0;31m# Do we have a slicer (on rows)?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3540\u001b[0m             \u001b[0;31m#  pending resolution of GH#33047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3542\u001b[0;31m             \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3543\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3544\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_col_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2894\u001b[0m                     \u001b[0;34m\"backfill or nearest lookups\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m                 )\n\u001b[0;32m-> 2896\u001b[0;31m             \u001b[0mcasted_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2897\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_maybe_cast_indexer\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4982\u001b[0m         \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mint\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mequivalent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4983\u001b[0m         \"\"\"\n\u001b[0;32m-> 4984\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4985\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4986\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mis_floating\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m         \"\"\"\n\u001b[0;32m-> 1796\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"floating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mixed-integer-float\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"integer-na\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36minferred_type\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mstring\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtype\u001b[0m \u001b[0minferred\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \"\"\"\n\u001b[0;32m-> 2005\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2007\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcache_readonly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}